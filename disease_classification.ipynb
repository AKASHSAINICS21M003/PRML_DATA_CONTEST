{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-11T08:47:30.213712Z","iopub.execute_input":"2021-11-11T08:47:30.214124Z","iopub.status.idle":"2021-11-11T08:47:30.225672Z","shell.execute_reply.started":"2021-11-11T08:47:30.214094Z","shell.execute_reply":"2021-11-11T08:47:30.224654Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/lib/kaggle/gcp.py\n/kaggle/input/datacontest-prml/Dataset_1_Training.csv\n/kaggle/input/datacontest-prml/Dataset_2_Testing.csv\n/kaggle/input/datacontest-prml/Dataset_1_Testing.csv\n/kaggle/input/datacontest-prml/Dataset_2_Training.csv\n/kaggle/input/prml-data-contest-jul-2021-rb-section/dummy_submission.csv\n/kaggle/working/__notebook_source__.ipynb\n","output_type":"stream"}]},{"cell_type":"code","source":"# dataset path\ntrain1_path = '/kaggle/input/datacontest-prml/Dataset_1_Training.csv'\ntrain2_path = '/kaggle/input/datacontest-prml/Dataset_2_Training.csv'\ntest1_path = '/kaggle/input/datacontest-prml/Dataset_1_Testing.csv'\ntest2_path = '/kaggle/input/datacontest-prml/Dataset_2_Testing.csv'\n\n# random seed\nseed = 1","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:30.376457Z","iopub.execute_input":"2021-11-11T08:47:30.376774Z","iopub.status.idle":"2021-11-11T08:47:30.380963Z","shell.execute_reply.started":"2021-11-11T08:47:30.376745Z","shell.execute_reply":"2021-11-11T08:47:30.380313Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def load_data(path):\n    df = pd.read_csv(path)\n    df = df.iloc[:, 1:]\n    return df.T\n\ndef load_train_data(path, till, lstart=1):\n    df = load_data(path)\n    X, Y = df.iloc[:, :till].reset_index(drop=True), df.iloc[:, till:].reset_index(drop=True)\n    _, nlabels = Y.shape\n    names = {(till+i):f'CO{i+lstart}' for i in range(0, nlabels)}\n    Y = Y.rename(columns=names)\n    return X, Y \n\ndef load_test_data(path):\n    df = load_data(path)\n    X = df.reset_index(drop=True)\n    return X","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:30.515445Z","iopub.execute_input":"2021-11-11T08:47:30.515894Z","iopub.status.idle":"2021-11-11T08:47:30.523055Z","shell.execute_reply.started":"2021-11-11T08:47:30.515847Z","shell.execute_reply":"2021-11-11T08:47:30.522458Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def normalize(X, std=True):\n    X = X - X.mean(axis=0)\n    X = X/X.std() if std else X\n    return X","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:30.636088Z","iopub.execute_input":"2021-11-11T08:47:30.636637Z","iopub.status.idle":"2021-11-11T08:47:30.640906Z","shell.execute_reply.started":"2021-11-11T08:47:30.636602Z","shell.execute_reply":"2021-11-11T08:47:30.640191Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def class_balance(X, Y, seed=1):\n    x, y = [X], [Y]\n    max_value = Y.value_counts().max()\n    for cl, grp in X.groupby(Y):\n        sample_size = max_value-len(grp)\n        if sample_size > 0:\n            x.append(grp.sample(sample_size, replace=True, random_state=int(cl)+seed))\n            y.append(pd.Series([cl for i in range(sample_size)]))\n    x = pd.concat(x).sample(frac=1, random_state=seed).reset_index(drop=True)\n    y = pd.concat(y).sample(frac=1, random_state=seed).reset_index(drop=True)\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:30.746828Z","iopub.execute_input":"2021-11-11T08:47:30.747240Z","iopub.status.idle":"2021-11-11T08:47:30.754124Z","shell.execute_reply.started":"2021-11-11T08:47:30.747211Z","shell.execute_reply":"2021-11-11T08:47:30.753533Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Training Dataset 1\nX1, Y1 = load_train_data(train1_path, till=22283)\nCO1, CO2 = Y1['CO1'], Y1['CO2']\n# Xtrain1, Xtest1 = normalize(Xtrain1), normalize(Xtest1)\n\n# Training Dataset 2\nX2, Y2 = load_train_data(train2_path, till=54675, lstart=3)\nCO3, CO4, CO5, CO6 = Y2['CO3'], Y2['CO4'], Y2['CO5'], Y2['CO6']\n# Xtrain2, Xtest2 = normalize(Xtrain2), normalize(Xtest2)\n\n# Test Dataset 1\nX1_test = load_test_data(test1_path)\n# Test Dataset 2\nX2_test = load_test_data(test2_path)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:30.887884Z","iopub.execute_input":"2021-11-11T08:47:30.888645Z","iopub.status.idle":"2021-11-11T08:47:39.565904Z","shell.execute_reply.started":"2021-11-11T08:47:30.888603Z","shell.execute_reply":"2021-11-11T08:47:39.564996Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\ndef do_variance_thres(train, test, th=0.1):\n    var = VarianceThreshold(threshold=th)\n    var.fit(train)\n    new_train = pd.DataFrame(data=var.transform(train), index = train.index)\n    new_test = pd.DataFrame(data=var.transform(test), index = test.index)\n    return (new_train, new_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:39.568007Z","iopub.execute_input":"2021-11-11T08:47:39.568368Z","iopub.status.idle":"2021-11-11T08:47:39.814037Z","shell.execute_reply.started":"2021-11-11T08:47:39.568326Z","shell.execute_reply":"2021-11-11T08:47:39.813329Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\n\ndef do_univariate(xtrain, xtest, ytrain, n_features=10):\n    uni = SelectKBest(k=n_features)\n    uni.fit(xtrain, ytrain)\n    new_train = pd.DataFrame(data=uni.transform(xtrain), index = xtrain.index)\n    new_test = pd.DataFrame(data=uni.transform(xtest), index = xtest.index)\n    return (new_train, new_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:39.815107Z","iopub.execute_input":"2021-11-11T08:47:39.815342Z","iopub.status.idle":"2021-11-11T08:47:39.821898Z","shell.execute_reply.started":"2021-11-11T08:47:39.815312Z","shell.execute_reply":"2021-11-11T08:47:39.821023Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"## Preprocessing\nfrom sklearn.decomposition import PCA\n\ndef do_pca(train, test, n=10):\n    pca = PCA(n_components=n, whiten=True, random_state=seed).fit(train)\n    new_train = pd.DataFrame(data=np.dot(train, pca.components_.T),\n                             index = train.index)\n    new_test = pd.DataFrame(data = np.dot(test, pca.components_.T),\n                            index = test.index)\n    return (new_train, new_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:39.824593Z","iopub.execute_input":"2021-11-11T08:47:39.825464Z","iopub.status.idle":"2021-11-11T08:47:39.832707Z","shell.execute_reply.started":"2021-11-11T08:47:39.825420Z","shell.execute_reply":"2021-11-11T08:47:39.831969Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, StandardScaler\n\ndef do_max_abs_scaling(train, test):\n    mu = train.mean(axis=0)\n    scale = MaxAbsScaler().fit(train-mu)\n    new_train = pd.DataFrame(data=scale.transform(train-mu), index=train.index)\n    new_test = pd.DataFrame(data=scale.transform(test-mu), index=test.index)\n    return (new_train, new_test)\n\ndef do_min_max_scaling(train, test):\n    scale = MinMaxScaler().fit(train)\n    new_train = pd.DataFrame(data=scale.transform(train), index=train.index)\n    new_test = pd.DataFrame(data=scale.transform(test), index=test.index)   \n    return (new_train, new_test)\n\ndef do_standard(train, test):\n    scale = StandardScaler().fit(train)\n    new_train = pd.DataFrame(data=scale.transform(train), index=train.index)\n    new_test = pd.DataFrame(data=scale.transform(test), index=test.index)   \n    return (new_train, new_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:39.833831Z","iopub.execute_input":"2021-11-11T08:47:39.834156Z","iopub.status.idle":"2021-11-11T08:47:39.845431Z","shell.execute_reply.started":"2021-11-11T08:47:39.834119Z","shell.execute_reply":"2021-11-11T08:47:39.844641Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"## Preprocessing\n## Very expensive way of feature selection\n\n# from sklearn.feature_selection import SequentialFeatureSelector\n# from sklearn.linear_model import LogisticRegression\n\n# def forward_step_selection(model, X, Y):\n#     sfs = SequentialFeatureSelector(model, n_features_to_select=0.2, direction='forward', scoring='balanced_accuracy')\n#     sfs.fit(X, Y)\n#     return sfs\n\n# model = LogisticRegression(random_state=seed, C=1.0)\n# sfs = forward_step_selection(model, Xtrain1, Ytrain1.iloc[:,0])","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:39.846557Z","iopub.execute_input":"2021-11-11T08:47:39.846846Z","iopub.status.idle":"2021-11-11T08:47:39.859019Z","shell.execute_reply.started":"2021-11-11T08:47:39.846819Z","shell.execute_reply":"2021-11-11T08:47:39.858184Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, balanced_accuracy_score, matthews_corrcoef\n\ndef num_mistakes(pred, expected):\n    miss = 0\n    for p,e in zip(pred, expected):\n        if p != e:\n            miss += 1\n    total = len(expected)\n    return miss, total","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:39.860226Z","iopub.execute_input":"2021-11-11T08:47:39.860659Z","iopub.status.idle":"2021-11-11T08:47:39.869977Z","shell.execute_reply.started":"2021-11-11T08:47:39.860629Z","shell.execute_reply":"2021-11-11T08:47:39.869258Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\ndef test_preprocessing(Xtrain, Ytrain, kwargs):\n    pca = kwargs.pop('pca', None)\n    var = kwargs.pop('var', None)\n    uni = kwargs.pop('uni', None)\n    max_scale = kwargs.pop('max_scale', False)\n    range_scale = kwargs.pop('range_scale', False)\n    std_scale = kwargs.pop('std_scale', False)\n    Xtest = kwargs.pop('test_data', pd.DataFrame())\n    assert not Xtest.empty\n    if pca != None:\n        num_features = None if pca == -1 else pca # None here means use all features\n        Xtrain, Xtest = do_pca(Xtrain, Xtest, num_features)\n    if var != None:\n        Xtrain, Xtest = do_variance_thres(Xtrain, Xtest, var)\n    if uni != None:\n        Xtrain, Xtest = do_univariate(Xtrain, Xtest, Ytrain, uni)\n    if max_scale:\n        Xtrain, Xtest = do_max_abs_scaling(Xtrain, Xtest)\n    if range_scale:\n        Xtrain, Xtest = do_min_max_scaling(Xtrain, Xtest)\n    if std_scale:\n        Xtrain, Xtest = do_standard(Xtrain, Xtest)\n    return (Xtrain, Xtest)\n\ndef with_cross_validation(model_func):\n    def wrap(X, Y, label, *args, **kwargs):\n        is_test = kwargs.pop('test', False)\n        rescale = kwargs.pop('rescale', True)\n        if is_test:\n            Xtrain, Xtest = test_preprocessing(X, Y, kwargs)\n            Ytrain = Y\n            if rescale:\n                upsample_X, upsample_Y = class_balance(Xtrain, Ytrain, seed=20)\n            else:\n                upsample_X, upsample_Y = Xtrain, Ytrain\n            assert upsample_X.shape[0] == upsample_Y.shape[0]\n            model, model_name = model_func(upsample_X, upsample_Y, *args, **kwargs)\n            Ytrain_pred = model.predict(Xtrain)\n            print(f'{model_name} MCC: {matthews_corrcoef(Ytrain, Ytrain_pred)}')\n            Ypred = model.predict(Xtest)\n            assert Xtest.shape[0] == Ypred.shape[0]\n            return Ypred\n        \n        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n        avg_test_mcc = 0; cnt = 0\n        avg_train_mcc = 0\n        pca = kwargs.pop('pca', None)\n        var = kwargs.pop('var', None)\n        uni = kwargs.pop('uni', None)\n        max_scale = kwargs.pop('max_scale', False)\n        range_scale = kwargs.pop('range_scale', False)\n        std_scale = kwargs.pop('std_scale', False)\n        for train_index, test_index in skf.split(X, Y):\n            Xtrain, Xtest, Ytrain, Ytest = X.iloc[train_index], X.iloc[test_index], Y.iloc[train_index], Y.iloc[test_index]\n            assert Xtrain.shape[0] == Ytrain.shape[0]\n            assert Xtest.shape[0] == Ytest.shape[0]\n            \n            if pca != None:\n                num_features = None if pca == -1 else pca # None here means use all features\n                Xtrain, Xtest = do_pca(Xtrain, Xtest, num_features)\n            if var != None:\n                Xtrain, Xtest = do_variance_thres(Xtrain, Xtest, var)\n            if uni != None:\n                Xtrain, Xtest = do_univariate(Xtrain, Xtest, Ytrain, uni)\n            if max_scale:\n                Xtrain, Xtest = do_max_abs_scaling(Xtrain, Xtest)\n            if range_scale:\n                Xtrain, Xtest = do_min_max_scaling(Xtrain, Xtest)\n            if std_scale:\n                Xtrain, Xtest = do_standard(Xtrain, Xtest)\n            if rescale:\n                upsample_X, upsample_Y = class_balance(Xtrain, Ytrain, seed=cnt)\n            else:\n                upsample_X, upsample_Y = Xtrain, Ytrain\n    \n            assert upsample_X.shape[0] == upsample_Y.shape[0]\n            model, model_name = model_func(upsample_X, upsample_Y, *args, **kwargs)\n            Ytrain_pred = model.predict(Xtrain)\n            train_mcc = matthews_corrcoef(Ytrain, Ytrain_pred)\n            avg_train_mcc += train_mcc\n            Ypred = model.predict(Xtest)\n            test_mcc = matthews_corrcoef(Ytest, Ypred)\n            avg_test_mcc += test_mcc; cnt += 1;\n            cm = confusion_matrix(Ytest, Ypred)\n            print(f'{model_name} Train MCC: {train_mcc}, Test MCC: {test_mcc}')\n#             print(f'Confusion matrix for {label}')\n#             print(cm)\n        print(f'{model_name} Average test MCC Score: {avg_test_mcc / cnt}')\n        print(f'{model_name} Average train MCC Score: {avg_train_mcc / cnt}')\n        print('---------------------------------------------')\n    return wrap","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:39.871557Z","iopub.execute_input":"2021-11-11T08:47:39.871844Z","iopub.status.idle":"2021-11-11T08:47:39.897753Z","shell.execute_reply.started":"2021-11-11T08:47:39.871806Z","shell.execute_reply":"2021-11-11T08:47:39.896784Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# SVM RBF Kernel\n\nfrom sklearn.svm import SVC\n\n@with_cross_validation\ndef svm(X, Y, C=1.0, kernel='rbf', degree=3, weight='balanced'):\n    mdl = SVC(kernel=kernel, cache_size=300, C=C, degree=degree, class_weight=weight, random_state=seed)\n    mdl.fit(X, Y)\n    return mdl, \"SVM\"","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:39.899162Z","iopub.execute_input":"2021-11-11T08:47:39.899669Z","iopub.status.idle":"2021-11-11T08:47:39.913412Z","shell.execute_reply.started":"2021-11-11T08:47:39.899625Z","shell.execute_reply":"2021-11-11T08:47:39.912573Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\n@with_cross_validation\ndef ada_boost(X, Y, n_esti=50, lr=1.0, base_esti=None):\n    adb = AdaBoostClassifier(base_estimator=base_esti, n_estimators=n_esti, learning_rate=lr, random_state=seed)\n    adb.fit(X, Y)\n    return adb, \"AdaBoost\"","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:39.915598Z","iopub.execute_input":"2021-11-11T08:47:39.915802Z","iopub.status.idle":"2021-11-11T08:47:39.974122Z","shell.execute_reply.started":"2021-11-11T08:47:39.915777Z","shell.execute_reply":"2021-11-11T08:47:39.973469Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# logistic regression\n\nfrom sklearn.linear_model import LogisticRegression\n\n@with_cross_validation\ndef logistic_regression(X, Y, reg=1.0):\n    lr = LogisticRegression(random_state=seed, C=reg, solver='liblinear', penalty='l1')\n    lr.fit(X, Y)\n    return lr, \"LogisticRegression\"","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:39.975059Z","iopub.execute_input":"2021-11-11T08:47:39.975408Z","iopub.status.idle":"2021-11-11T08:47:39.980480Z","shell.execute_reply.started":"2021-11-11T08:47:39.975374Z","shell.execute_reply":"2021-11-11T08:47:39.979639Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\n@with_cross_validation\ndef naive_bayes(X, Y):\n    nb = GaussianNB()\n    nb.fit(X, Y)\n    return nb, \"NaiveBayes\"","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:39.981497Z","iopub.execute_input":"2021-11-11T08:47:39.982188Z","iopub.status.idle":"2021-11-11T08:47:39.995939Z","shell.execute_reply.started":"2021-11-11T08:47:39.982124Z","shell.execute_reply":"2021-11-11T08:47:39.995184Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Linear Discriminant Analysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n@with_cross_validation\ndef linear_discriminant(X, Y):\n    lda = LinearDiscriminantAnalysis()\n    lda.fit(X, Y)\n    return lda, \"LDA\"","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:39.997353Z","iopub.execute_input":"2021-11-11T08:47:39.997566Z","iopub.status.idle":"2021-11-11T08:47:40.014049Z","shell.execute_reply.started":"2021-11-11T08:47:39.997533Z","shell.execute_reply":"2021-11-11T08:47:40.013486Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n@with_cross_validation\ndef random_forest(X, Y, n_esti=100, depth=2, features=0.5, samples=0.5):\n    rf = RandomForestClassifier(n_estimators=n_esti,\n                                criterion='entropy',\n                                max_depth=depth,\n                                max_features=features,\n                                bootstrap=True,\n                                class_weight='balanced',\n                                max_samples=samples,\n                                random_state=seed\n                               )\n    rf.fit(X, Y)\n    return rf, \"RandomForest\"","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:40.079149Z","iopub.execute_input":"2021-11-11T08:47:40.079743Z","iopub.status.idle":"2021-11-11T08:47:40.085769Z","shell.execute_reply.started":"2021-11-11T08:47:40.079702Z","shell.execute_reply":"2021-11-11T08:47:40.085161Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\n@with_cross_validation\ndef gradient_classifier(X, Y, n_esti=100, depth=2, sample=1.0, lr=1.0, features=0.5):\n    gb = GradientBoostingClassifier(n_estimators = n_esti,\n                                    learning_rate = lr,\n                                    subsample = sample,\n                                    max_depth = depth,\n                                    max_features = features,\n                                    random_state = seed\n                                   )\n    gb.fit(X, Y)\n    return gb, \"GradientBoosting\"","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:47:40.531521Z","iopub.execute_input":"2021-11-11T08:47:40.531809Z","iopub.status.idle":"2021-11-11T08:47:40.537925Z","shell.execute_reply.started":"2021-11-11T08:47:40.531778Z","shell.execute_reply":"2021-11-11T08:47:40.537316Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# MCC: 0.432 (pca=20)\n# logistic_regression(X1, CO1, 'CO1', pca=20, rescale=True)\n# MCC: 0.353\n# logistic_regression(X1, CO1, 'CO1', var=0.5)\n# MCC: 0.391\n# logistic_regression(X1, CO1, 'CO1', uni=5)\n\n# MCC: 0.367\n# naive_bayes(X1, CO1, 'CO1', pca=10)\n# MCC: 0.398\n# naive_bayes(X1, CO1, 'CO1', var=1.0)\n# MCC: 0.496 (uni=45)\n# naive_bayes(X1, CO1, 'CO1', uni=45, rescale=True)\n\n# MCC: 0.466\n# linear_discriminant(X1, CO1, 'CO1', pca=20)\n# MCC: 0.471\n# linear_discriminant(X1, CO1, 'CO1', var=0.5)\n# MCC: 0.403\n# linear_discriminant(X1, CO1, 'CO1', uni=5)\n\n# MCC: 0.438 (max_depth=2, max_features=0.5, max_samples=0.5)\n# random_forest(X1, CO1, 'CO1', pca=10)\n# MCC: 0.429 (max_depth=2, max_features=0.5, max_samples=0.5)\n# random_forest(X1, CO1, 'CO1', var=1.5)\n# MCC: 0.449\n# random_forest(X1, CO1, 'CO1', uni=60)\n\n# MCC: 0.418 (max_depth = 2, max_features = 0.5, subsample = 1.0)\n# gradient_classifier(X1, CO1, 'CO1', pca=40)\n# MCC: (not good performance) (max_depth = 2, max_features = 0.5, subsample = 1.0)\n# gradient_classifier(X1, CO1, 'CO1', var=1.7)\n# MCC: 0.483\n# gradient_classifier(X1, CO1, 'CO1', uni=65, lr=0.7)\n\n# MCC: 0.532 (kernel='rbf', uni=40, C=0.5, range_scale=True, rescale=False)\nsvm(X1, CO1, 'CO1', kernel='rbf', uni=40, C=0.5, range_scale=True, rescale=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T18:31:34.862191Z","iopub.execute_input":"2021-11-10T18:31:34.863071Z","iopub.status.idle":"2021-11-10T18:31:36.889028Z","shell.execute_reply.started":"2021-11-10T18:31:34.863014Z","shell.execute_reply":"2021-11-10T18:31:36.887765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CO1.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MCC: 0.427\n# logistic_regression(X1, CO2, 'CO2', pca=10)\n# MCC: 0.424\n# logistic_regression(X1, CO2, 'CO2', var=0.5)\n\n# MCC: 0.357\n# naive_bayes(X1, CO2, 'CO2', pca=10)\n# MCC: 0.508\n# naive_bayes(X1, CO2, 'CO2', uni=20, rescale=True)\n\n# MCC: 0.364\n# linear_discriminant(X1, CO2, 'CO2', pca=10)\n# MCC: 0.379\n# linear_discriminant(X1, CO2, 'CO2', var=0.9)\n# MCC: 0.445\n# linear_discriminant(X1, CO2, 'CO2', uni=10)\n\n# MCC: 0.399 (max_depth=2, max_features=0.5, max_sample=0.5)\n# random_forest(X1, CO2, 'CO2', pca=10)\n# MCC: 0.498\n# random_forest(X1, CO2, 'CO2', uni=50)\n\n# MCC: 0.470\n# gradient_classifier(X1, CO2, 'CO2', pca=40)\n# MCC: 0.472\n# gradient_classifier(X1, CO2, 'CO2', uni=15, lr=0.5)\n\n# MCC: 0.551 (kernel='poly', degree=3, uni=15, C=1.0, max_scale=True, rescale=False)\n# svm(X1, CO2, 'CO2', kernel='poly', degree=3, uni=15, C=1.0, max_scale=True, rescale=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T15:55:39.438993Z","iopub.execute_input":"2021-11-10T15:55:39.439519Z","iopub.status.idle":"2021-11-10T15:55:39.527017Z","shell.execute_reply.started":"2021-11-10T15:55:39.439428Z","shell.execute_reply":"2021-11-10T15:55:39.525835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CO2.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MCC: 0.316\n# logistic_regression(X2, CO3, 'CO3', pca=40)\n\n# MCC: 0.295\n# naive_bayes(X2, CO3, 'CO3', pca=10)\n# MCC: 0.294\n# naive_bayes(X2, CO3, 'CO3', var=2.0)\n# MCC: 0.285\n# naive_bayes(X2, CO3, 'CO3', uni=40)\n\n# MCC: 0.304\n# linear_discriminant(X2, CO3,'CO3', pca=20)\n# MCC: 0.329\n# linear_discriminant(X2, CO3,'CO3', var=2.0)\n\n# MCC: 0.311 (max_depth=2, max_features=0.5, max_sample=0.5)\n# random_forest(X2, CO3, 'CO3', pca=20)\n# MCC: 0.310\nrandom_forest(X2, CO3, 'CO3', uni=30)\n\n# MCC: 0.219\n# gradient_classifier(X2, CO3, 'CO3', n_esti=50, depth=1, uni=50, rescale=False)\n\n# MCC: 0.361 (pca=25, kernel='poly', degree=2, C=0.8, range_scale=True, rescale=False)\n# svm(X2, CO3, 'CO3', pca=25, kernel='poly', degree=2, C=0.8, range_scale=True, rescale=False)\n\n# MCC: 0.354 (uni=35, kernel='rbf', C=0.9, weight={1:2.0, 0:0.7}, range_scale=True, rescale=False)\n# svm(X2, CO3, 'CO3', uni=35, kernel='rbf', C=0.9, weight={1:2.0, 0:0.7}, range_scale=True, rescale=False)\n\n# MCC: 0.324\n# ada_boost(X2, CO3, 'CO3', n_esti=50, uni=25, lr=0.1, rescale=False)\n\n# MCC: 0.303 (n_esti=20, uni=35, lr=0.5, rescale=False)\n# ada_boost(X2, CO3, 'CO3', n_esti=20, uni=35, lr=0.5, rescale=False)\n\n# MCC:  Doesnot perform well\n# ada_boost(X2, CO3, 'CO3', n_esti=10, lr=0.5, rescale=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T15:56:31.2128Z","iopub.execute_input":"2021-11-10T15:56:31.213655Z","iopub.status.idle":"2021-11-10T15:56:40.817057Z","shell.execute_reply.started":"2021-11-10T15:56:31.213613Z","shell.execute_reply":"2021-11-10T15:56:40.816145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(CO3) / (2*np.bincount(CO3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CO3.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MCC: 0.098\n# logistic_regression(X2, CO4, 'CO4', pca=30)\n# MCC: 0.130 (var in the mcc is high)\n# logistic_regression(X2, CO4, 'CO4', var=3.5)\n\n# MCC: 0.195\n# naive_bayes(X2, CO4, 'CO4', pca=15)\n# MCC: 0.218\n# naive_bayes(X2, CO4, 'CO4', var=3.5, rescale=True)\n# MCC: 0.206 (15)\n# naive_bayes(X2, CO4, 'CO4', uni=15, rescale=True)\n\n# MCC: 0.182\n# linear_discriminant(X2, CO4, 'CO4', pca=15)\n# MCC: 0.205\n# linear_discriminant(X2, CO4, 'CO4', var=2.5)\n\n# MCC: 0.204 (pca=15, n_esti=100, depth=2, features=0.5, samples=0.5)\n# random_forest(X2, CO4,'CO4', pca=15, n_esti=100, depth=2, features=0.5, samples=0.5, rescale=False)\n\n# MCC: 0.112\n# gradient_classifier(X2, CO4, 'CO4', pca=25)\n# MCC: 0.203 (uni=25, n_esti=50, depth=1, sample=0.5, lr=1.0, features=0.5, range_scale=True, rescale=True)\n# gradient_classifier(X2, CO4, 'CO4', uni=25, n_esti=50, depth=1, sample=0.5, lr=1.0, features=0.5, range_scale=True, rescale=True)\n\n# MCC: 0.142\n# svm(X2, CO4, 'CO4', uni=25, kernel='rbf', degree=2, C=0.6, max_scale=True, rescale=False)\n\n# MCC: \n# svm(X2, CO4, 'CO4', uni=15, kernel='rbf', degree=2, C=1.0, range_scale=True, rescale=False)\n\n# MCC: 0.231 (uni=25, n_esti=200, lr=0.1, std_scale=True, rescale=False)\nada_boost(X2, CO4, 'CO4', uni=25, n_esti=200, lr=0.1, std_scale=True, rescale=False)\n\n# from sklearn.tree import DecisionTreeClassifier\n# clf = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=1, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=1, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n# ada_boost(X2, CO4, 'CO4', base_esti=clf, n_esti=60, pca=12, lr=1.0, rescale=True)\n\n# MCC: All data doesnot perform well\n# ada_boost(X2, CO4, 'CO4', n_esti=10, lr=0.5, rescale=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T16:28:05.164314Z","iopub.execute_input":"2021-11-10T16:28:05.164984Z","iopub.status.idle":"2021-11-10T16:28:15.852531Z","shell.execute_reply.started":"2021-11-10T16:28:05.164947Z","shell.execute_reply":"2021-11-10T16:28:15.851715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(CO4) / (2*np.bincount(CO4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(X2, CO4, test_size=0.3, random_state=100)\nnew_train, new_test = do_variance_thres(xtrain, xtest, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z = new_train[ytrain == 0]\no = new_train[ytrain==1]\n\np = z.plot(kind='scatter', x=3, y=5, color='r')\no.plot(kind='scatter', x=3, y=5, color='b', ax=p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MCC: 0.846\n# logistic_regression(X2, CO5, 'CO5', uni=10, rescale=True)\n\n# MCC: 0.427\n# naive_bayes(X2, CO5, 'CO5', pca=-1)\n\n# MCC: 0.153\n# linear_discriminant(X2, CO5, 'CO5', pca=20)\n\n# MCC: 0.82 (max_depth=2, max_features=0.3) # cal. using only 1 run\n# MCC: 0.440 (max_depth=2, max_features=0.5, pca=50)\n# random_forest(X2, CO5, 'CO5', pca=50)\n\n# MCC: 0.838\n# svm(X2, CO5, 'CO5', kernel='rbf', C=1.0, uni=10, max_scale=True, rescale=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T16:38:10.616164Z","iopub.execute_input":"2021-11-10T16:38:10.616487Z","iopub.status.idle":"2021-11-10T16:38:19.088865Z","shell.execute_reply.started":"2021-11-10T16:38:10.616452Z","shell.execute_reply":"2021-11-10T16:38:19.087922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MCC: -0.057\n# logistic_regression(X2, CO6, 'CO6', pca=-1)\n# MCC: 0.089 (high var in score)\n# logistic_regression(X2, CO6, 'CO6', var=2.0)\n\n# MCC: 0.085 (variance of the model is very high, but sometimes it yeilds good performance)\n# naive_bayes(X2, CO6, 'CO6', pca=15)\n# MCC: 0.069\n# naive_bayes(X2, CO6, 'CO6', var=0.5)\n\n# MCC: 0.091\n# linear_discriminant(X2, CO6, 'CO6', pca=60, std_scale= True)\n# MCC: 0.092\n# linear_discriminant(X2, CO6, 'CO6', var=0.5)\n# MCC: 0.122\n# linear_discriminant(X2, CO6, 'CO6', uni=10, std_scale=True, rescale=True)\n\n# MCC: 0.065 (max_depth=2, max_features=0.5)\n# random_forest(X2, CO6, 'CO6', pca=10, n_esti=100, depth=2, features=0.5, samples=0.5, rescale=False)\n# MCC: 0.113 (high var in score)\n# random_forest(X2, CO6, 'CO6', uni=30)\n\n# MCC: 0.088\n# gradient_classifier(X2, CO6, 'CO6', pca=10)\n# MCC: 0.142 (uni=25, n_esti=200, depth=1, sample=0.5, lr=0.5, features=0.5, std_scale=True)\n# gradient_classifier(X2, CO6, 'CO6', uni=25, n_esti=100, depth=1, sample=0.5, lr=0.5, features=0.5, std_scale=True, rescale=True)\n\n# MCC: \n# svm(X2, CO6, 'CO6', uni=10, kernel='rbf', degree=2, C=2.0, std_scale=True, rescale=False)\n\n# MCC:\nsvm(X2, CO6, 'CO6', uni=25, kernel='rbf', C=1.0, weight={1:0.8, 0:1.2}, range_scale=True, rescale=False)\n\n# MCC: 0.152 (n_esti=10, uni=150, std_scale=True, rescale=False)\n# ada_boost(X2, CO6, 'CO6', n_esti=10, uni=150, std_scale=True, rescale=False)\n\n# MCC:\n# ada_boost(X2, CO6, 'CO6', n_esti=40, pca=75, lr=0.6, std_scale=True, rescale=True)\n\n# MCC: All data training very poor performance\n# ada_boost(X2, CO6, 'CO6', n_esti=10, lr=0.5, rescale=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(CO6)/(2*np.bincount(CO6))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission 1 (0.365) [attempt_final1]\n# CO1 : 0.532\ntest_CO1 = svm(X1, CO1, 'CO1', kernel='rbf', uni=40, C=0.5, range_scale=True, rescale=False, test=True, test_data=X1_test).astype(int)\n\n# CO2 : 0.551\ntest_CO2 = svm(X1, CO2, 'CO2', kernel='poly', degree=3, uni=15, C=1.0, max_scale=True, rescale=False, test=True, test_data=X1_test).astype(int)\n\n# CO3 : 0.316\ntest_CO3 = logistic_regression(X2, CO3, 'CO3', pca=40, test=True, test_data=X2_test).astype(int)\n\n# CO4 : 0.218\ntest_CO4 = naive_bayes(X2, CO4, 'CO4', var=3.5, rescale=True, test=True, test_data=X2_test).astype(int)\n\n# CO5 : 0.838\ntest_CO5 = svm(X2, CO5, 'CO5', kernel='rbf', C=1.0, uni=10, max_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# CO6 : 0.122\ntest_CO6 = linear_discriminant(X2, CO6, 'CO6', uni=10, std_scale=True, test=True, test_data=X2_test).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission 2\n# CO1 : 0.532\ntest_CO1 = svm(X1, CO1, 'CO1', kernel='rbf', uni=40, C=0.5, range_scale=True, rescale=False, test=True, test_data=X1_test).astype(int)\n\n# CO2 : 0.551\ntest_CO2 = svm(X1, CO2, 'CO2', kernel='poly', degree=3, uni=15, C=1.0, max_scale=True, rescale=False, test=True, test_data=X1_test).astype(int)\n\n# CO3 : 0.361\ntest_CO3 = svm(X2, CO3, 'CO3', pca=25, kernel='poly', degree=2, C=0.8, range_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# CO4 : 0.231\ntest_CO4 = ada_boost(X2, CO4, 'CO4', uni=25, n_esti=100, lr=0.1, std_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# CO5 : 0.838\ntest_CO5 = svm(X2, CO5, 'CO5', kernel='rbf', C=1.0, uni=10, max_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# CO6 : 0.114\ntest_CO6 = ada_boost(X2, CO6, 'CO6', n_esti=10, uni=150, std_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission 3 (Not Good diff from 1st is CO4)\n# CO1 : 0.532\ntest_CO1 = svm(X1, CO1, 'CO1', kernel='rbf', uni=40, C=0.5, range_scale=True, rescale=False, test=True, test_data=X1_test).astype(int)\n\n# CO2 : 0.551\ntest_CO2 = svm(X1, CO2, 'CO2', kernel='poly', degree=3, uni=15, C=1.0, max_scale=True, rescale=False, test=True, test_data=X1_test).astype(int)\n\n# CO3 : 0.316\ntest_CO3 = logistic_regression(X2, CO3, 'CO3', pca=40, test=True, test_data=X2_test).astype(int)\n\n# CO4 : 0.231\ntest_CO4 = ada_boost(X2, CO4, 'CO4', uni=25, n_esti=100, lr=0.1, std_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# CO5 : 0.838\ntest_CO5 = svm(X2, CO5, 'CO5', kernel='rbf', C=1.0, uni=10, max_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# CO6 : 0.122\ntest_CO6 = linear_discriminant(X2, CO6, 'CO6', uni=10, std_scale=True, test=True, test_data=X2_test).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission 4 :[0.358, seed:567, attempt_final2.csv; 0.3804, seed:1, attempt_final4.csv] \n# CO1 : 0.532\ntest_CO1 = svm(X1, CO1, 'CO1', kernel='rbf', uni=40, C=0.5, range_scale=True, rescale=False, test=True, test_data=X1_test).astype(int)\n\n# MCC: 0.508\ntest_CO2 = naive_bayes(X1, CO2, 'CO2', uni=20, rescale=True, test=True, test_data=X1_test).astype(int)\n\n# MCC: 0.361\ntest_CO3 = svm(X2, CO3, 'CO3', pca=25, kernel='poly', degree=2, C=0.8, range_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# MCC: 0.218\ntest_CO4 = naive_bayes(X2, CO4, 'CO4', var=3.5, rescale=True, test=True, test_data=X2_test).astype(int)\n\n# MCC: 0.832\ntest_CO5 = svm(X2, CO5, 'CO5', kernel='rbf', C=1.0, uni=10, max_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# MCC: 0.122\ntest_CO6 = linear_discriminant(X2, CO6, 'CO6', uni=10, std_scale=True, test=True, test_data=X2_test).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission 5 : 0.367, seed=1 [attempt_final3.csv] \n# CO1 : 0.532\ntest_CO1 = svm(X1, CO1, 'CO1', kernel='rbf', uni=40, C=0.5, range_scale=True, rescale=False, test=True, test_data=X1_test).astype(int)\n\n# MCC: 0.508\ntest_CO2 = naive_bayes(X1, CO2, 'CO2', uni=20, rescale=True, test=True, test_data=X1_test).astype(int)\n\n# MCC: 0.361\ntest_CO3 = svm(X2, CO3, 'CO3', pca=25, kernel='poly', degree=2, C=0.8, range_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# MCC: 0.231\ntest_CO4 = ada_boost(X2, CO4, 'CO4', uni=25, n_esti=100, lr=0.1, range_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# MCC: 0.832\ntest_CO5 = svm(X2, CO5, 'CO5', kernel='rbf', C=1.0, uni=10, max_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# MCC: 0.122\ntest_CO6 = linear_discriminant(X2, CO6, 'CO6', uni=10, std_scale=True, rescale=True, test=True, test_data=X2_test).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission 6: 0.38109 [seed 1, attempt_final5] [FINAL SUBMITTED: 1]\n# CO1 : 0.532\ntest_CO1 = svm(X1, CO1, 'CO1', kernel='rbf', uni=40, C=0.5, range_scale=True, rescale=False, test=True, test_data=X1_test).astype(int)\n\n# MCC: 0.508\ntest_CO2 = naive_bayes(X1, CO2, 'CO2', uni=20, rescale=True, test=True, test_data=X1_test).astype(int)\n\n# MCC: 0.354\ntest_CO3 = svm(X2, CO3, 'CO3', uni=35, kernel='rbf', C=0.9, weight={1:2.0, 0:0.7}, range_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# MCC: 0.218\ntest_CO4 = naive_bayes(X2, CO4, 'CO4', var=3.5, rescale=True, test=True, test_data=X2_test).astype(int)\n\n# MCC: 0.832\ntest_CO5 = svm(X2, CO5, 'CO5', kernel='rbf', C=1.0, uni=10, max_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# MCC: 0.122\ntest_CO6 = linear_discriminant(X2, CO6, 'CO6', uni=10, std_scale=True, test=True, test_data=X2_test).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:50:22.161542Z","iopub.execute_input":"2021-11-11T08:50:22.161891Z","iopub.status.idle":"2021-11-11T08:50:31.284523Z","shell.execute_reply.started":"2021-11-11T08:50:22.161858Z","shell.execute_reply":"2021-11-11T08:50:31.283464Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"SVM MCC: 0.6403346309638315\nNaiveBayes MCC: 0.5603272937579981\nSVM MCC: 0.5962169403415911\nNaiveBayes MCC: 0.43014327602168023\nSVM MCC: 0.870609213762706\nLDA MCC: 0.38782723005395864\n","output_type":"stream"}]},{"cell_type":"code","source":"# Submission 7 : 0.367, seed=1 [attempt_final6.csv] [FINAL SUBMITTED: 2]\n# CO1 : 0.532\ntest_CO1 = svm(X1, CO1, 'CO1', kernel='rbf', uni=40, C=0.5, range_scale=True, rescale=False, test=True, test_data=X1_test).astype(int)\n\n# MCC: 0.508\ntest_CO2 = naive_bayes(X1, CO2, 'CO2', uni=20, rescale=True, test=True, test_data=X1_test).astype(int)\n\n# MCC: 0.354\ntest_CO3 = svm(X2, CO3, 'CO3', uni=35, kernel='rbf', C=0.9, weight={1:2.0, 0:0.7}, range_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# MCC: 0.231\ntest_CO4 = ada_boost(X2, CO4, 'CO4', uni=25, n_esti=100, lr=0.1, range_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# MCC: 0.832\ntest_CO5 = svm(X2, CO5, 'CO5', kernel='rbf', C=1.0, uni=10, max_scale=True, rescale=False, test=True, test_data=X2_test).astype(int)\n\n# MCC: 0.122\ntest_CO6 = linear_discriminant(X2, CO6, 'CO6', uni=10, std_scale=True, rescale=True, test=True, test_data=X2_test).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:50:59.068468Z","iopub.execute_input":"2021-11-11T08:50:59.068767Z","iopub.status.idle":"2021-11-11T08:51:07.417790Z","shell.execute_reply.started":"2021-11-11T08:50:59.068738Z","shell.execute_reply":"2021-11-11T08:51:07.416757Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"SVM MCC: 0.6403346309638315\nNaiveBayes MCC: 0.5603272937579981\nSVM MCC: 0.5962169403415911\nAdaBoost MCC: 0.7269133380753248\nSVM MCC: 0.870609213762706\nLDA MCC: 0.38782723005395864\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\noutput_dir = '/kaggle/working'\nfilename = 'attempt_final6.csv'\nresults = [test_CO1, test_CO2, test_CO3, test_CO4, test_CO5, test_CO6]\nwith open(os.path.join(output_dir, filename), 'w') as fp:\n    cnt = 0\n    fp.write(f'Id,Predicted\\n')\n    for res in results:\n        for r in res:\n            fp.write(f'{cnt},{r}\\n')\n            cnt += 1","metadata":{"execution":{"iopub.status.busy":"2021-11-11T08:51:07.419781Z","iopub.execute_input":"2021-11-11T08:51:07.420209Z","iopub.status.idle":"2021-11-11T08:51:07.430018Z","shell.execute_reply.started":"2021-11-11T08:51:07.420154Z","shell.execute_reply":"2021-11-11T08:51:07.428948Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"CO4.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nXtrain, Xtest, Ytrain, Ytest = train_test_split(X2, CO6, test_size=0.3)\nxtrain, xtest, ytrain, ytest = train_test_split(Xtrain, Ytrain, test_size=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for e in rates:\n    model = GradientBoostingClassifier(learning_rate=e)\n    model.fit(xtrain, ytrain)\n    \n    train_pred = model.predict(xtrain)\n    train_result.append(matthews_corrcoef(ytrain, train_pred))\n    \n    test_pred = model.predict(xtest)\n    test_result.append(matthews_corrcoef(ytest, test_pred))\n\n\nimport matplotlib.pyplot as plt\n\nline1 = plt.plot(rates, train_result, 'b', label='Train MCC')\nline2 = plt.plot(rates, test_result, 'r', label='Test MCC')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndef search(model, parms):\n    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X2, CO6, test_size=0.3, random_state=154)\n    Xtrain, Xtest = do_univariate(Xtrain, Xtest, Ytrain, 30)\n\n    clf = GridSearchCV(model, parameters, cv=10, scoring='balanced_accuracy')\n    clf.fit(Xtrain, Ytrain)\n    print(f'Best Params: {clf.best_params_}')\n    print(f'Best Score: {clf.best_score_}')\n    print(f'Train MCC: {matthews_corrcoef(Ytrain, clf.predict(Xtrain))}')\n    print(f'Test MCC: {matthews_corrcoef(Ytest, clf.predict(Xtest))}')\n    print(f'Test balance acc: {balanced_accuracy_score(Ytrain, clf.predict(Xtrain))}')\n    print(f'Test balance acc: {balanced_accuracy_score(Ytest, clf.predict(Xtest))}')\n    return clf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select parms for Random Forest Algo\n\nparameters = {'n_estimators': [25, 50, 100],\n              'max_samples': [0.5, 0.7, 0.99],\n              'max_features': [0.2, 0.5, 0.7],\n              'max_depth': [2, 5, 10, 15]\n             }\n\nmodel = RandomForestClassifier(criterion='entropy')\nclf = search(model, parameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gradient Boosting\n\nparameters = {'learning_rate': [0.5, 0.75],\n              'n_estimators': [50, 150],\n              'subsample': [ 0.5, 0.75, 1.0],\n              'max_depth': [2, 5],\n              'max_features': [0.7, 1.0]\n             }\nmodel = GradientBoostingClassifier()\nclf = search(model, parameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gb = GradientBoostingClassifier(learning_rate=, n_estimators=, subsample=, max_depth=, max_features=)\nypred = clf.predict(Xtest)\nmatthews_corrcoef(Ytest, ypred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier(criterion='entropy',max_depth=10, max_features=0.7, max_samples=0.7, n_estimators=100)\nrf.fit(Xtrain, Ytrain)\nypred = rf.predict(Xtest)\nmatthews_corrcoef(Ytest, ypred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
